\documentclass[pdf, 10pt, aspectratio=169, bigger, unicode]{beamer}


\usepackage[utf8]{inputenc}
\usepackage[T1, T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{pscyr}

\usepackage{graphics}
\usepackage{graphicx}
\usepackage{graphpap}
\usepackage{ragged2e}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{indentfirst}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage{multicol}
\usepackage{slashbox}
\usepackage{tabularx}
\usepackage{bibunits}
\usepackage{epstopdf}
\usepackage{algorithmicx}

 
\usepackage{float}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}



\usefonttheme[onlymath]{serif}
\usetikzlibrary{decorations}
\usetikzlibrary{arrows,automata}
\usetikzlibrary{positioning}

\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}
%Нужно включать, если используется "тема" (стиль оформления) по умолчанию
\usepackage{beamerthemesplit}
\usetheme{Warsaw}


\usecolortheme{seahorse}


%\captionsetup[ruled]{labelsep=period}

\renewcommand{\thealgorithm}{\arabic{algorithm}}
\floatname{algorithm}{Алгоритм}
\algrenewcommand\algorithmicrequire{\textbf{Вход: }}
\algrenewcommand\algorithmicensure{\textbf{Выход: }}
\algrenewcommand\algorithmicwhile{\textbf{До тех пока}}
	\algrenewcommand\algorithmicdo{\textbf{выполнять}}
	\algrenewcommand\algorithmicrepeat{\textbf{Повторять}}
	\algrenewcommand\algorithmicuntil{\textbf{Пока выполняется}}
	\algrenewcommand\algorithmicend{\textbf{Конец}}
	\algrenewcommand\algorithmicif{\textbf{Если}}
	\algrenewcommand\algorithmicelse{\textbf{иначе}}
	\algrenewcommand\algorithmicthen{\textbf{тогда}}
	\algrenewcommand\algorithmicfor{\textbf{Для}}
	\algrenewcommand\algorithmicforall{\textbf{Выполнить для всех}}
	\algrenewcommand\algorithmicfunction{\textbf{Функция}}
	\algrenewcommand\algorithmicprocedure{\textbf{Процедура}}
	\algrenewcommand\algorithmicloop{\textbf{Зациклить}}
	\algrenewcommand\algorithmicreturn{\textbf{Возвратить}}
	\algrenewtext{EndWhile}{\textbf{Конец цикла}}
	\algrenewtext{EndLoop}{\textbf{Конец зацикливания}}
	\algrenewtext{EndFor}{\textbf{Конец цикла}}
	\algrenewtext{EndFunction}{\textbf{Конец функции}}
	\algrenewtext{EndProcedure}{\textbf{Конец процедуры}}
	\algrenewtext{EndIf}{\textbf{Конец условия}}
	\algrenewtext{EndFor}{\textbf{Конец цикла}}
	\algrenewtext{BeginAlgorithm}{\textbf{Начало алгоритма}}
	\algrenewtext{EndAlgorithm}{\textbf{Конец алгоритма}}
	\algrenewtext{BeginBlock}{\textbf{Начало блока. }}
	\algrenewtext{EndBlock}{\textbf{Конец блока}}
	\algrenewtext{ElsIf}{\textbf{иначе если }}
		
\algloop{description}
	\algnewcommand\algorithmicdescription{\textbf{Описание алгоритма}}


\mode<presentation>
{
    \usefonttheme[onlymath]{serif}
		%\usetheme{Copenhagen}
        %\usetheme{Warsaw}
        %\usetheme{Darmstadt}
        %\usetheme{Frankfurt}
        %\usetheme{AnnArbor}
        %\usetheme{CambridgeUS}
    \setbeamercovered{transparent}	
	\setbeamertemplate{footline}
{%
 \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm,rightskip=.3cm]{author in head/foot}%
            \usebeamerfont{author in head/foot}%\insertframenumber{}/\inserttotalframenumber%
                        \hfill\insertshortauthor
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
						\hfill \insertframenumber{}/\inserttotalframenumber%
        \end{beamercolorbox}}%
    \vskip0pt%
}
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{navigation symbols}{}
}

\bibliographystyle{unsrt}
\makeatletter
\addto\captionsrussian{
    \renewcommand{\figurename}{image}
    %\def\figurename{Рисунок}
}


%Более крупный шрифт для подзаголовков титульного листа
\setbeamerfont{institute}{size=\normalsize}

%Задание команды (\bluetext) для выделения конкретным (синим) цветом
%(используйте \alert для выделения цветом выбранной "темы")
\setbeamercolor{bluetext_color}{fg=blue}
\newcommand{\bluetext}[1]{{\usebeamercolor[fg]{bluetext_color}#1}}

\renewcommand{\raggedright}{\leftskip=0pt \rightskip=0pt plus 0cm}

\renewcommand{\rmdefault}{ftm}

%Если используется последовательное появление пунктов списков на слайде
%(не злоупотребляйте в слайдах для защиты дипломной работы), чтобы
%еще непоявившиеся пункты были все-таки немножко видны.
\setbeamercovered{transparent}

\title[MARL FOR TRAFFIC LIGHT NETWORK]{MARL FOR TRAFFIC LIGHT NETWORK}
\author[Тисленко Т.И.]{ \bf Тисленко Тимофей Иванович}
\institute[ИМиФИ СФУ]{	{\footnotesize ФГАОУ ВО <<СИБИРСКИЙ ФЕДЕРАЛЬНЫЙ УНИВЕРСИТЕТ>>\\

    Scool of mathematics and computer science\\[-2pt]
		
	%Кафедра высшей и прикладной математики
	}

 % \vspace{0.5cm}
%{\footnotesize Направление \ 01.037.02 Прикладная математика }

  \vspace{0.2cm}
    {\footnotesize Научный руководитель --- к.ф.-м.н.,  доцент   {\sf Д.В. Семенова }					}
 }
\date[\today]{\footnotesize Томск, ITMM	 2021}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\titlepage

 \end{frame}

\normalsize
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{THE WORK RELEVANCE}
    \justifying
    There is  a fast motorist comunityfast growing in Krasnoyarsk. According to ''AUTOSTAT'' agency's data our city is on 12 place by such growing. 
    As the number of motorists increases, so does the time spent in traffic jams.
\end{frame}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
    \frametitle{The goals and tasks}
    \justifying
    \begin{block}{The work's goals}
        \justifying
        The development and study of a mathematical model of a multi-agent system for the problem of optimization of traffic at a crossroads.
    \end{block}

    %\pause

    \begin{block}{Задачи}
        \begin{enumerate} \justifying
            \item Review literature on relevant topics.
            \item Construct the mathematical MARL model .
            \item Develop an algorithm for solving the MARL problem.
            \item Do a computational experiments.
        \end{enumerate}
    \end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \justifying
    \frametitle{Main defiunitions and deskriptions}
    \begin{block}{defiunitions} 
        \begin{enumerate} \justifying
            \item	\textbf{The intellectual agent} is called a meta-object with a partiality of subjectivity, which is interacting with other agents and the environment, performing certain functions to achieve it's objectives.
            \item	\textbf{The environment} is called the set of objects that are not  belong to the agent.
            \item 
            \item 	\textbf{The multiagent system} –-- set of related agents.
            \item 	\textbf{RL(Reinforcement Learning)} is a learning where environment is a teacher.
        \end{enumerate}
    \end{block}
					


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\justifying
\frametitle{Problem statement}


    \begin{block}\justifying
        \begin{enumerate}[-] \justifying    
            \item The model is a Markov homogeneous chain with finite number of actions

            $\textbf{A}$ and states $\textbf{S}$ and discrete time $t$;
            \item environment --- Section of the road network where the time for cars to pass through intersections is calculated according the rule: the time count begins at 100m before stop lines of traffic lights;
            \item set of agents $K$ --- all traffic lights on the road network;
            \item reward  --- total calculated time of vehicles passing through sections of the road at time $t$;
            \item $p_{s s'}$ --- the probability that the $s$, when choosing a $a$ solution, falls into the $s'$ state is entirely determined by the state in which the process is going.
            \item the phases of the traffic lights change sequentially;
        \end{enumerate}
    \end{block}




\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\justifying
\frametitle{Problem statement}

    \begin{block}\justifying
        \begin{enumerate}[-] \justifying    
            \item state space of agent $k$ $S^k$  --- is a set defined by a residue class modulo $n^k=|S^k|$: $S^k = n^k\mathbb{Z}  = \left\{s^{(0)} = 0, s^{(1)} = 1, \ldots, s^{(n^k-1)} = n^k-1\right\}$,  where  $s^{(i)}$ interpreted as  <<the phase $i$ is active>>;
            \item the set of solutions is defined by the main characteristic of the ring $n^k\mathbb{Z}$, as it is not bigger than\dots,
            $A =\left\{a^{(0)} = 0, a^{(1)} = 1, \ldots, a^{(n^k-1)} = n^k-1 \right\}$, where $a^{(j)}$ interpreted as  
        <<change phase $j$ times>>. 
            
        \end{enumerate}
    \end{block}

    


    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
        semithick]
    \tikzstyle{every state}=[draw,text=black]
    
    \node[state] (A)  {$s^{(0)}$};
    \node[state] (B) [right of=A] {$s^{(1)}$};
    
    
    \path (A) edge[bend right] node {$p_{01}$} (B);
    \path (B) edge[bend right] node {$p_{10}$} (A);
    \path (A) edge[loop left, dashed] node {$p_{00}$} (A);
    \path (B) edge[loop right, dashed] node {$p_{11}$} (B);
    \end{tikzpicture}
        \caption{The stochastic graph of a controllable phase change process for a two-phase traffic light.  Dashed to indicate action $a^{(0)}$, and continious --- $a^{(1)}$.
        }
        \label{fig:stohgraph}
    \end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\justifying
\frametitle{Problem statement}


\begin{block}\justifying
    \begin{enumerate}[-] \justifying    
        \item combined state of the environment $\textbf{s}_t$ at time $t$ is \linebreak $\textbf{s}_t = \{s_t^1, s_t^2, \ldots, s_t^K\}\in S^1\times S^2\times\ldots\times S^K$ ---  ;
        \item combined action of the environment $\textbf{a}_t$ at time $t$ is  $\textbf{a}_t = \{a_t^1, a_t^2, \ldots, a_t^K\} \in A^1\times A^2\times\ldots\times A^K$; 
        \item the set $N(k)$ is set of $k$ neibours --- the agents with which it interacts $k$;
        \item the pair $k$ and $j\in N(k)$ is a set  $\textbf{a}^{kj} \in A^k\times A^j$ and combined states $\textbf{s}^{kj} \in S^k\times S^j$;
        \item reward $r(s_t, a_t; s_{t+1})$ at phase $s_t$  we understand the total recorded time of all cars passing through a section of road.
        
    \end{enumerate}
\end{block}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \justifying
    \frametitle{Problem statement}
    
    
    \begin{figure}[h!]
        %\centering
        %\includegraphics[width =0.2\textwidth]{images/intersection2.png}
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.8cm,
          semithick, scale=0.1]
      \tikzstyle{every state}=[draw,text=black]
      
      \node[state] (s0)  {$s_{0}$};
      \node[state] (s1) [right of=s0] {$s_{1}$};
      \node[state] (s2) [below  of=s0] {$s_{2}$};
      \node[state] (s3) [below  of=s1] {$s_{3}$};
      
      \path (s0) edge[sloped,midway,above, bend left] node {$p_{01}, a^{(1)}$} (s1);
      \path (s1) edge[sloped,midway,above] node {$p_{10}, a^{(1)}$} (s0);
      
      \path (s2) edge[sloped,midway,above, bend left] node {$p_{20}, a^{(2)}$} (s0);
      \path (s0) edge[sloped,midway,below] node {$p_{02}, a^{(2)}$} (s2);
      
      
      \path (s1) edge[sloped,midway,above, bend left] node {$p_{13}, a^{(2)}$} (s3);
      \path (s3) edge[sloped,midway,above] node {$p_{31}, a^{(1)}$} (s1);
      %\path (s1) edge[sloped,midway,above, bend left= 320] node {$p_{12}, a^{(3)}$} (s2);
      
      \path (s2) edge[sloped,midway,below] node {$p_{23}, a^{(1)}$} (s3);
      \path (s3) edge[sloped,midway,below,bend left] node {$p_{32}, a^{(1)}$} (s2);
      
      \path (s0) edge[sloped,midway,above, bend left] node {$p_{03}, a^{(3)}$} (s3);
      \path (s3) edge[sloped,midway,below, bend left] node {$p_{30}, a^{(3)}$} (s0);
      
      \path (s2) edge[sloped,midway,above,dashed, bend left] node {$p_{21}, a^{(3)}$} (s1);
      \path (s1) edge[sloped,midway,below,dashed, bend left] node {$p_{12}, a^{(3)}$} (s2);
      
      
      %\path (s2) edge[sloped,midway,above, bend below of =s3] node {$p_{21}, a^{(3)}$} (s1);
      
      \path (s0) edge[loop left, dashed] node {$p_{00}, a^{(0)} $} (s0);
      \path (s1) edge[loop right, dashed] node {$p_{00}, a^{(0)}$} (s1);
      \path (s2) edge[loop left, dashed] node {$p_{00}, a^{(0)}$} (s2);
      \path (s3) edge[loop right, dashed] node {$p_{00}, a^{(0)}$} (s3);
      
      %\path (s1) edge[loop right, dashed] node {$p_{11}$} (s1);
      \end{tikzpicture}
        \includegraphics[scale=0.2]{images/intersection2.png}
        \caption{A stochastic network graph consisting of two two-phase traffic lights.}
        \label{fig:stohgraph3}
      \end{figure}
    \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\justifying
\frametitle{Defenitions and deskriptions}
    
    
    The Agent cumulative Earnings/Rewards function of the selected and fixed strategy  $ \delta$ will take the form
    \begin{equation}\label{VALUE}
        V\Bigl(\{S_t, \delta\}\Bigr) = \sum_{t=0}^{\infty}\gamma^t r(s_t,a_t),
    \end{equation}
        where $0 \leq \gamma \leq 1$ --- revaluation factor, $\{S_t\}$ --- randomizing $\xi(t)$ with the chosen strategy $\delta =\{a_t, 0\leq t<\infty\}$.
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\justifying
\frametitle{MARL for one traffic light}
    \begin{block}{Setting the MARL problem for one traffic light}\justifying
        Need to find such a control $\delta$, that provides max of the function
        $$
        V\Bigl(\{S_t, \delta\}\Bigr),
        $$
        where function 
        \begin{equation}
            V^*\Bigl(\{S_t\}\Bigr)=\max\limits_\delta Q(s, a), 
        \end{equation}
        and $Q(s, a)$ is 
        \begin{equation}
            Q(s,a)  =  \sum_{s'\in S} p(s,a;s')\bigl(r(s,a;s')+\gamma \max_{a \in A}Q(s',a') \bigr).
        \end{equation}
        \end{block}

    
	\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Convergence criterion}
\justifying

    The main idea of the $Q$-learning is valuation of the indeterminate right part of equation:
    \begin{equation} \label{Qiteration}
        Q_{t+1}(s, a) =  Q_t(s, a) + \alpha _t (s, a) \left(r(s, a) +  \gamma \max_{a'\in A}  Q_t(s', a') - Q_t(s, a)\right)
    \end{equation}
    where $s'$ --- stage situation $t+1$, on step $t$ process was at $s$ and took action $a$,
    $\alpha$ --- sales factor.

        If on step $t$ iteration of process is at state $s$ and action $a$, is taken then $0 \leq \alpha _t(s, a) \leq 1$, else $\alpha _t(s, a) = 0$.

 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Convergence criterion}
\justifying
    
	$Q = \{ Q(s, a)\}_{s \in S, a' \in A}$ , iteratively is $Q_{t+1} = A(Q_t)$, 
    
    where $A \colon \mathbb{R} _{\infty}^1 \to  \mathbb{R} _{\infty}^1$ --- Contraction mapping.
    \begin{multline*}
    \rho ((A \circ Q_1)(s, a), (A \circ Q_2)(s, a)) % = \\
     %   = \max_{a'\in A, s'\in S}| \sum_{s'\in S}  p(s, a; s')(r(s, a) + \gamma \max_{a'\in A}  Q_1(s', a')) - \\
   %  - \sum_{s'\in S}  p(s, a; s')(r(s, a) + \gamma \max_{a'\in A}  Q_2(s', a'))|  
    \leq  \max |\gamma \max_{a'\in A}  Q_1(s', a')) - \gamma \max_{a'\in A}  Q_2(s', a'))| = \\
        = 	\gamma \rho (Q_1(s, a), Q_2(s, a)) ,  \gamma \in (0; 1)
    \end{multline*}

    \begin{block}{Convergence criterion for one traffic light MARL problem}
        If strategy $\delta$ is optimal, then
        using contraction mapping condition and
    \begin{equation}
        \sum _{t=0} ^\infty {\alpha _t (s, a)} = \infty, \qquad
        \sum _{t=0} ^\infty {\alpha _t (s, a)^2} \leq \infty
    \end{equation}
    attracts convergence of Q-process(\ref{Qiteration})
    \end{block}
	

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Solution of MARL problem for one trafficlight}
\justifying

    The MARL solution is 
        \begin{equation}
	    V ^*(s) = \max_{a\in A}  \lim _{t \to + \infty}Q_t(s, a).
    \end{equation}
    \begin{equation}
	    a_t(s) = \arg  \max_{a' \in A} Q_t(s, a')
    \end{equation}	

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}
% \frametitle{Алгоритм}
% \justifying

% \begin{figure}[h!]
%     \centering
        
%         \includegraphics[scale = 0.5]{images/blol.png}
%         \label{fig:block}
%         \caption{4. Блок-схема алгоритма}
% \end{figure}

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Algorithm}
    \justifying
    
    \begin{figure}[h!]
        \centering
            
            \includegraphics[scale = 0.5]{images/code.png}
            \label{fig:Qfactor_code}
            \caption{5. The <<Qfactor>> code }
    \end{figure}
    
    
    
    
    \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Computational experiments}
    \justifying
    
    \begin{block}{goals}\justifying
        Compare the delay time of cars in the model of traffic light control system whose phase duration is obtained by overdrive,
        and managed by the Markov process, in the Anylogic system.
    \end{block}

    \begin{block}{inputs} \justifying  
        \begin{enumerate}[-] \justifying
            \item Cars arrive at the intersection from each of three directions at a rate of 1,000 per hour;
            \item Discount and reassessment coefficients are empirically selected.
        \end{enumerate}
    \end{block}

    \begin{block}{outputs} \justifying  
        \begin{enumerate}[-] \justifying
            \item Acceleration 1.5 times that of the traffic light control system, the phases of which are selected by a step of 1 from 5 seconds to 30 seconds;
        \end{enumerate}
    \end{block}


\end{frame}
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Results}
\justifying
The purpose of the work was to introduce the approach to optimize the selection of a traffic light, taking into account the current load of transport, from the standpoint of minimizing delay,
as well as creating an algorithm that implements this approach and calculating delays in its navigation.
The results are as follows:

\begin{enumerate}
	\item A mathematical model of the process of selecting the phase of a traffic light is constructed, which differs by taking into account the current position of traffic lights and their loading and allows to formulate optimization tasks, the purpose of which is to minimize the delay of traffic of cars.
	\item A multi-agent system structure has been developed, which includes a single agent - a traffic light, which ensures the most efficient parallelization of the entire task on sub-tasks that will be solved by the agent.
\end{enumerate}	


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{References}\justifying
    \begin{thebibliography}{9}
        \bibitem{BOOK1}
            El-Tantawy S., Abdulhai B. and Abdelgawad H., Multiagent Reinforcement Learning for Integrated Network of Adaptive Traffic Signal Controllers (MARLIN-ATSC) // IEEE Transactions on Intelligent Transportation Systems, vol. 14, no. 3. 2013. -P 1140-1150.
        \bibitem{BOOK2}
            Лекции по случайным процессам: учебное пособие / А. В. Гасников, Э. А. Горбунов, С. А. Гуз и др.; под ред. А. В. Гасникова. – «Москва»: МФТИ, 2019. – 285 с. 
        \bibitem{BOOK3}
            Марковские процессы принятия решений. / Майн X., Осаки С. Главная редакция физико-математической литературы издательства «Наука», 1977. – 176 с. 
        \bibitem{BOOK4}
            Sandholm T.W. Contract Types for Satisficing Task Allocation: I Theoretical Results // AAAI Spring Symposium Series: Satisficing Models. 1998. – P. 68-75.
        %
    \end{thebibliography}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
\begin{frame}
\begin{center}

{\color{blue}{\Huge{\bf THANKS FOR ATTENTION!!!}}}
\end{center}
\end{frame}



\end{document}

